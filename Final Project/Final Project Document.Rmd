---
title: "Final Project Document"
author: "Vardan Martirosyan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Calling Libraries that we need.
```{r}
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(ISLR)
require(gridExtra)
```


#Importing the csv file.
```{r}
dataset <- read.csv('/Users/vardan/Desktop/pstat131/Final\ Project/student-mat.csv')
```



\section*{Starting Exploratory Data Analysis}
First, we want to analyze how the three variables, G1, G2, G3 change over time, as G1 is the grades at the first grading period, G2 is the grades at the second grading period, and G3 is the grades at the final grading period (and the variable we want to predict). We use the grid.arrange() function as follows to compare the three variables side by side and see how grades shift over time. 
```{r}
plot1 <- ggplot(data = dataset, aes(x = G1)) + geom_histogram(color = 'black', fill = 'red', bins = 15)
plot2 <- ggplot(data = dataset, aes(x = G2)) + geom_histogram(color = 'black', fill = 'green', bins = 15)
plot3 <- ggplot(data = dataset, aes(x = G3)) + geom_histogram(color = 'black', fill = 'blue', bins = 15)

grid.arrange(plot1, plot2, plot3, ncol = 3)

```
Looking at these graphs over time, we see an interesting trend. In the first grading period, the distribution of G1 is mostly uniform, but there seems to be quite a lot of students who have low class grades (lower than the mean of 10). In the second grading period, we start to see an uptick in the number of students who have a 0 as a class grade (but the number of students who had a lower class grade than 10 has decreased slightly). In the final grading period, there is an even larger uptick in the number of students who have a class grade of 0, but now there are very few students who have a non-zero class grader that is lower than the mean of 10. Analyzing this, I think that this means the following. As students begin the class, very few students drop the class by the first grading period, even if they are not performing well. However, as the end of the quarter/semester approaches, students who are not performing as well may choose to cut their losses and drop the class. This means that their class grade becomes a 0. Thus, I think that this is the reason why we see so many students who have a score of 0 for G3, their final score in the class: they were the students who dropped the class because they weren't performing well/other outside circumstances.

Now, we want to get rid of all the nominal predictors in the dataset, and see how it looks like with just numeric predictors. Note that we don't include G1 and G2, since they are clearly related to G3, and we would run into an issue of collinearity.
```{r}
dataset <- dataset %>% select(c(age, Medu, Fedu, traveltime, studytime, 
                                failures, famrel, freetime, goout, Dalc, 
                                Walc, health, absences, G3))
```


Now that we have reduced the dataset just to numerical predictors that shouldn't cause an issue with collinearity, we can perform linear regression. 
```{r}
#Setting the seed.
set.seed(69)


#spliitting the dataset into two for a training and a test set.
dataset_split <- initial_split(dataset, prop = 0.70, strata = G3)

#creating the training and testing sets.
dataset_train <- training(dataset_split)
dataset_test <- testing(dataset_split)

#creating the recipe.
recipe <-
  recipe(G3 ~ ., data = dataset_train) %>%
  step_normalize(all_predictors()) %>%
  step_center(all_predictors())


#creating the linear regression object.
lm_model <- linear_reg() %>%
  set_engine("lm")

#creating the workflow.
lm_wflow <- workflow() %>%
  add_model(lm_model) %>%
  add_recipe(recipe)

#fit the linear model according to our training set.
lm_fit <- fit(lm_wflow, dataset_train)

#viewing the results of this.
results <- lm_fit %>%
  # This will return the parsnip object.
  extract_fit_parsnip() %>%
  # Now we tidy the linear model object.
  tidy()

results
```
Looking at the estimates for each of the variables, we can notice some interesting things. One, it seems that as the variables 1) age, 2) the amount of time you spending travelling to school, 3) the number of previous class failures, and 4) the amount of time you spend going out with friends, increases, the lower your overall class grade will be. On the other hand, we can see that as the 1) the mother's education level, 2) study time, 3) free time, and 4) weekend alcohol consumption increases, the higher your overall class grade will be. 

Most of these trends seem to make sense, with two needing maybe some explanation. I think the reason that the mother's education level being higher leads to an increase in the overall grade, as opposed to the father's, is as follows. Traditinally, mother's are the parents that stay at home with children to take care of them. This means that they spend more time with their children, and help them with things such as homework. Thus, kids who have mother's that are more educated are more likely to have a better education than those that do not. So, if a kid has a mother that is highly educated, than she may be able to help them more in their studies (and get a better grade) than a mother who has not had the chance to obtain a high education. 

The other trend that doesn't make much sense is weekend alcohol consumption level leading to higher grades. My theory is as follows: we can see from the predictors that the higher your weekday alcohol consumption level is, the lower overall grade you might have. Thus, it would make sense that if students who drink a lot on the weekends, but not a lot on the weekdays, have higher final scores, as opposed to students that do the opposite. It might be an indication that students who only drink on the weekends prioritize their studies more, and thus, have higher overall grades.

Using this, we can then see how well this model classifies the observations in the dataset as follows.

```{r}
dataset_metrics <- metric_set(rmse, rsq, mae)

dataset_train_res <- predict(lm_fit, new_data = dataset_train %>% select(-G3))
dataset_train_res <- bind_cols(dataset_train_res, dataset_train %>% select(G3))

metrics <- dataset_metrics(dataset_train_res, truth = G3, estimate = .pred)

metrics
```

From this, we see that the R^2 value is pretty low, which means that only around 15.7 percent of the variability in the G3 value is explained by the predictors. Let's see if we can improve this any further, by perhaps including all of the categorical and nominal predictors in addition to the other predictors.








\section*{Linear Regression Including All Variables}
Now, we want to try doing linear regression, but including all of the variables instead of just the quantative ones. We do this as follows:
```{r}
dataset2 <- read.csv('/Users/vardan/Desktop/pstat131/Final\ Project/student-mat.csv')
dataset2 <- dataset2 %>% select(-c(G1,G2))

#Then, we turn the two nominal categories, Mjob and Fjob, into numeric columns.
dataset2['Mjob'] <- as.numeric(factor(as.matrix(dataset2['Mjob'])))
dataset2['Fjob'] <- as.numeric(factor(as.matrix(dataset2['Fjob'])))


#splitting the dataset into two for a training and a test set.
dataset2_split <- initial_split(dataset2, prop = 0.80, strata = G3)

#creating the training and testing sets.
dataset2_train <- training(dataset2_split)
dataset2_test <- testing(dataset2_split)

#creating the recipe.
recipe2 <-
  recipe(G3 ~ ., data = dataset2_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_center(all_predictors())


#creating the linear regression object.
lm_model2 <- linear_reg() %>%
  set_engine("lm")

#creating the workflow.
lm_wflow2 <- workflow() %>%
  add_model(lm_model2) %>%
  add_recipe(recipe2)

#fit the linear model according to our training set.
lm_fit2 <- fit(lm_wflow2, dataset2_train)

#viewing the results of this.
results2 <- lm_fit2 %>%
  # This will return the parsnip object.
  extract_fit_parsnip() %>%
  # Now we tidy the linear model object.
  tidy()

arrange(results2, estimate)





```
From this, we can see that failures, being in a romantic relationship, going out, and needing extra educational support are some of the stronger factors that impact someone getting a lower final grade. We can also see that mother's education, having a guardian other than the father/mother, and being male leads to a higher grade. Using this, we can then see how well this model classifies the observations in the dataset as follows.

```{r}
dataset_metrics2 <- metric_set(rmse, rsq, mae)

dataset2_train_res <- predict(lm_fit2, new_data = dataset2_train %>% select(-G3))
dataset2_train_res <- bind_cols(dataset2_train_res, dataset2_train %>% select(G3))

metrics2 <- dataset_metrics(dataset2_train_res, truth = G3, estimate = .pred)

metrics2
```
From this, we see that the R^2 value is pretty low, which means that only around 27 percent of the variability in the G3 value is explained by the predictors. Another thing to note is that the predicted values are not integer values, like the actual values are, which we can see below as follows:
```{r}
head(dataset2_train_res)
```

To resolve this, we then try doing logistic regression.

\section*{Logistic Regression Model}
```{r}
#First, we need to split up the response variable, G3, into two categories.
dataset3 <- read.csv('/Users/vardan/Desktop/pstat131/Final\ Project/student-mat.csv')
dataset3 <- dataset3 %>% select(-c(G1,G2))

#Then, we turn the G3 variable into a categorical variable.
#dataset['G3'] <- 

dataset3$G3 <- cut(dataset$G3,
                       breaks=c(0, 10, 20),
                       labels=c(0, 1))


#Then, we turn the two nominal categories, Mjob and Fjob, into numeric columns.
dataset2['Mjob'] <- as.numeric(factor(as.matrix(dataset3['Mjob'])))
dataset2['Fjob'] <- as.numeric(factor(as.matrix(dataset3['Fjob'])))


#splitting the dataset into two for a training and a test set.
dataset3_split <- initial_split(dataset3, prop = 0.80, strata = G3)

#creating the training and testing sets.
dataset3_train <- training(dataset3_split)
dataset3_test <- testing(dataset3_split)

#creating the recipe.
recipe3 <-
  recipe(G3 ~ ., data = dataset3_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_center(all_predictors())


#creating the logistic regression object.
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

#creating the workflow.
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(recipe3)

#fit the logistic model according to our training set.
log_fit <- fit(log_wkflow, dataset3_train)


#viewing the results of this.
results3 <- log_fit %>%
  # This will return the parsnip object.
  extract_fit_parsnip() %>%
  # Now we tidy the linear model object.
  tidy()

arrange(results3, estimate)
```
Then, we want to check the performance of this model.
```{r}
predict(log_fit, new_data = dataset3_train, type = "prob")

augment(log_fit, new_data = dataset3_train) %>%
  conf_mat(truth = G3, estimate = .pred_class)
```
This confusion matrix seems to indicate that we're performing pretty good with this logistic regression model, as opposed to the linear regression model. We calculate it's accuracy on the training dataset as follows:
```{r}
log_reg_acc <- augment(log_fit, new_data = dataset3) %>%
  accuracy(truth = G3, estimate = .pred_class)
log_reg_acc
```
It seems that we have approximately 76.5 percent training accuracy on this dataset. That's pretty good! We then test it on the testing data to see how it performs there.

```{r}
#Predicting on the testing data.
predict(log_fit, new_data = dataset3_test, type = "prob")

augment(log_fit, new_data = dataset3_test) %>%
  conf_mat(truth = G3, estimate = .pred_class)

multi_metric <- metric_set(accuracy, sensitivity, specificity)

augment(log_fit, new_data = dataset3_test) %>%
  multi_metric(truth = G3, estimate = .pred_class)
```
It looks like we have 61.76 accuracy, which is somewhat good, although not great. We then want to see how the other types of models we learned in class perform. We use the LDA, QDA, and Naive Bayes Models on the training data and check their accuracies. 
```{r}
#LDA WORK.
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#We then define the workflow and add the recipe.
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(recipe3)

#We then fit the data on the training data.
lda_fit <- fit(lda_wkflow, dataset3_train)

#getting the accuracy
lda_acc <- augment(lda_fit, new_data = dataset3_train) %>%
  accuracy(truth = G3, estimate = .pred_class)

#QDA WORK.
#Defining the model for QDA.
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#Creating the workflow and adding the recipe.
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(recipe3)

#Fitting the model.
qda_fit <- fit(qda_wkflow, dataset3_train)

qda_acc <- augment(qda_fit, new_data = dataset3_train) %>%
  accuracy(truth = G3, estimate = .pred_class)

#NAIVE BAYES WORK.
#We fit the naive Bayes model as asked of us.
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

#We create the workflow, and add our recipe into it.
nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(recipe3)

#We then fit our model on the training dataset.
nb_fit <- fit(nb_wkflow, dataset3_train)

#Getting the Naive Bayes accuracy.
nb_acc <- augment(nb_fit, new_data = dataset3_train) %>%
  accuracy(truth = G3, estimate = .pred_class)

#Getting the accuracy for logistic regression.
#log_reg_acc <- augment(log_fit, new_data = dataset3_train) %>%
#  accuracy(truth = G3, estimate = .pred_class)


#Putting all of the accuracies together.
accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)

#Putting all of the model names together.
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")

#Combining the two lists into one tibble.
results <- tibble(accuracies = accuracies, models = models)

#Printing out the results and arranging them nicely. 
results %>% 
  arrange(-accuracies)
```
Looking at this, we can see that on the training data, QDA seems to be performing the best, at a whopping 92.73 percent. Logistic Regression, LDA, and Naive Bayes all seem to be performing very closely to one another, ranging from 76.47 percent to 73.66 percent to 71.62 percent, respectively. However, this is all on the training data. Let us see how they perform on the testing data.
```{r}
#ISSUE CURRENTLY: CANNOT DO THIS ON TRAINING DATA BECAUSE ITS TOO SMALL?




#LDA WORK.
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#We then define the workflow and add the recipe.
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(recipe3)

#We then fit the data on the training data.
lda_fit <- fit(lda_wkflow, dataset3_test)

#getting the accuracy
lda_acc <- augment(lda_fit, new_data = dataset3_test) %>%
  accuracy(truth = G3, estimate = .pred_class)

#QDA WORK.
#Defining the model for QDA.
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#Creating the workflow and adding the recipe.
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(recipe3)

#Fitting the model.
qda_fit <- fit(qda_wkflow, dataset3_test)

qda_acc <- augment(qda_fit, new_data = dataset3_test) %>%
  accuracy(truth = G3, estimate = .pred_class)

#NAIVE BAYES WORK.
#We fit the naive Bayes model as asked of us.
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

#We create the workflow, and add our recipe into it.
nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(recipe3)

#We then fit our model on the training dataset.
nb_fit <- fit(nb_wkflow, dataset3_test)

#Getting the Naive Bayes accuracy.
nb_acc <- augment(nb_fit, new_data = dataset3_test) %>%
  accuracy(truth = G3, estimate = .pred_class)


#fit the logistic model according to our testing set.
log_fit <- fit(log_wkflow, dataset3_test)

#Getting the accuracy for logistic regression.
log_reg_acc <- augment(log_fit, new_data = dataset3_test) %>%
  accuracy(truth = G3, estimate = .pred_class)


#Putting all of the accuracies together.
accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)

#Putting all of the model names together.
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")

#Combining the two lists into one tibble.
results <- tibble(accuracies = accuracies, models = models)

#Printing out the results and arranging them nicely. 
results %>% 
  arrange(-accuracies)
```
We can see that all of our methods seem to perform quite a bit worse on the testing data, as opposed to the training data. Suprisingly, we see that Naive Bayes seems to perform the best, with a 70.58 percent, which is quite close to the 71.62 percent it reached on the training data. On the other hand, all the other methods are performing about 10 percent worse, with the exception of QDA, which is performing about 25 percent worse. Finally, while accuracy is a decent indicator of the performance of a model, we can also use the ROC curve to determine if a model is performing well or not. 



























