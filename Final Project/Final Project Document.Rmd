---
title: "Analyzing Final Scores in a Math Class"
author: "Vardan Martirosyan"
date: "`r Sys.Date()`"
output: html_document:
    code_folding: hide

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\section*{Introduction}
In this paper, we will be analyzing the "Math Students" dataset, obtained from Kaggle.com. In particular, the dataset comes from the UCI dataset repository, and contains information about the final scores of students at the conclusion of a math program, with many additional variables regarding the students themselves (such as their previous academic performances, characteristics, information about their parents, etc). The goal of the paper is to predict student's final grades using the information given in the dataset. More specifically, our goal is to predict if a student's grade is below 50 percent, or above 50 percent, at the end of the class. 

This model could have several uses in the future. One use is as follows: it could be used to predict the grades of incoming students into the math class based on the student's prior characteristics, and identify students who may need additional help in the class. This would lead to students who might normally not perform as well in the class to getting additional help, which could help them perform better in the class. Another use is as follows. The people surrounding students (such as teachers, parents, etc.) could look at which variables in the dataset seem to influence student's performances the most. If these variables are within the teachers and parents control, they could try to influence these variables ahead of time to improve the performance of their students in the class. These are just some uses that the model could have in the future. 


\section*{Loading Packages and Data}
First, we need to load in the packages and data that we plan on using. As mentioned above, we obtained our data from Kaggle. The link to this dataest is as follows: https://www.kaggle.com/datasets/janiobachmann/math-students

We note that the codebook for this paper is contained in the Github repository that was submitted as part of the final submission. We load in the packages and data that we plan on using as follows. Note that the code chunk is folded: all of the code chunks in this paper will be folded, unless otherwise specified beforehand.
```{r}
#Calling all of the libraries that we plan on using.
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(ISLR)
require(gridExtra)
library(cowplot)

#Importing the data as a .csv file.
dataset <- read.csv('/Users/vardan/Desktop/pstat131/Final\ Project/student-mat.csv')
```



\section*{Exploratory Data Analysis}
Before starting the main analysis, we want to confirm that there are no missing variables. We check the number of missing values in each variable of the dataset as follows.
```{r class.source = 'fold-show'}
#Using sapply to check the number of missing values in each variable of the dataset.
sapply(dataset, function(x) sum(is.na(x)))
```
From this, we verify that there are no missing values in the dataset. Thus, we don't need to deal with missing values in our exploratory data analysis. Additionally, we note that the original dataset contained binary categorical variables, numeric variables, and nominal variables. We choose to transform the nominal variables into numeric variables, so that there are only two variables types in our dataset (which makes not only our analysis, but prediction, a bit easier). We do this transformation below as follows:
```{r}
#Transforming the two nominal variables, Mjob and Fjob, into numeric variables.
dataset['Mjob'] <- as.numeric(factor(as.matrix(dataset['Mjob'])))
dataset['Fjob'] <- as.numeric(factor(as.matrix(dataset['Fjob'])))
```
Having accomplished this, we can now move on with the main Exploratory Data Analysis. 




\subsection*{Examining the Relationship Between G1, G2, G3}
First, we want to analyze how the three variables, G1, G2, G3 change over time, as G1 is the grades at the first grading period, G2 is the grades at the second grading period, and G3 is the grades at the final grading period (and the variable we want to predict). We use the grid.arrange() function as follows to compare the three variables side by side and see how grades shift over time. 
```{r}
#Creating the histogram plots for the variables G1, G2, G3.
plot1 <- ggplot(data = dataset, aes(x = G1)) + geom_histogram(color = 'black', fill = 'white', bins = 15) + labs(y = "Number of Students", title = "Grade Period 1 Scores")

plot2 <- ggplot(data = dataset, aes(x = G2)) + geom_histogram(color = 'black', fill = 'lightblue', bins = 15) + labs(y = "Number of Students", title = "Grade Period 2 Scores")

plot3 <- ggplot(data = dataset, aes(x = G3)) + geom_histogram(color = 'black', fill = 'darkblue', bins = 15) + labs(y = "Number of Students", title = "Grade Period 3 Scores")

#Arranging the plots together side-by-side so that they be analyzed together.
grid.arrange(plot1, plot2, plot3, ncol = 3)
```
Looking at these graphs over time, we see an interesting trend. In the first grading period, the distribution of G1 is mostly uniform, but there seems to be quite a lot of students who have low class grades (lower than the mean of 10). In the second grading period, we start to see an uptick in the number of students who have a 0 as a class grade (but the number of students who had a lower class grade than 10 has decreased slightly). In the final grading period, there is an even larger uptick in the number of students who have a class grade of 0, but now there are very few students who have a non-zero class grader that is lower than the mean of 10. Analyzing this, I think that this means the following. As students begin the class, very few students drop the class by the first grading period, even if they are not performing well. However, as the end of the quarter/semester approaches, students who are not performing as well may choose to cut their losses and drop the class. This means that their class grade becomes a 0. Thus, I think that this is the reason why we see so many students who have a score of 0 for G3, their final score in the class: they were the students who dropped the class because they weren't performing well/other outside circumstances.


\subsection*{Exploring Numerical Variables vs. G3}
Continuing onward with EDA, we now want to see how G3 relates to all of the numerical predictors in our dataset (excluding G1/G2, since they are related to G3). By viewing these scatterplots, we can see if there are any numeric predictors we can exclude based on the information that they (don't) give us. We do this as follows:
```{r, warning = FALSE, message = FALSE}
#Creating the plots for the numerical variables against G3.
plot4 <- ggplot(data = dataset, aes(x = age, y = G3)) + geom_point() + geom_smooth(method=lm)

plot5 <- ggplot(data = dataset, aes(x = Medu, y = G3)) + geom_point() + geom_smooth(method=lm)

plot6 <- ggplot(data = dataset, aes(x = Fedu, y = G3)) + geom_point() + geom_smooth(method=lm)

plot7 <- ggplot(data = dataset, aes(x = traveltime, y = G3)) + geom_point() + geom_smooth(method=lm)

plot8 <- ggplot(data = dataset, aes(x = studytime, y = G3)) + geom_point() + geom_smooth(method=lm)

plot9 <- ggplot(data = dataset, aes(x = failures, y = G3)) + geom_point() + geom_smooth(method=lm)

plot10 <- ggplot(data = dataset, aes(x = famrel, y = G3)) + geom_point() + geom_smooth(method=lm)

plot11 <- ggplot(data = dataset, aes(x = freetime, y = G3)) + geom_point() + geom_smooth(method=lm)

plot12 <- ggplot(data = dataset, aes(x = goout, y = G3)) + geom_point() + geom_smooth(method=lm)

plot13 <- ggplot(data = dataset, aes(x = Dalc, y = G3)) + geom_point() + geom_smooth(method=lm)

plot14 <- ggplot(data = dataset, aes(x = Walc, y = G3)) + geom_point() + geom_smooth(method=lm)

plot15 <- ggplot(data = dataset, aes(x = health, y = G3)) + geom_point() + geom_smooth(method=lm)

plot16 <- ggplot(data = dataset, aes(x = absences, y = G3)) + geom_point() + geom_smooth(method=lm)

plot17 <- ggplot(data = dataset, aes(x = Mjob, y = G3)) + geom_point() + geom_smooth(method=lm)

plot18 <- ggplot(data = dataset, aes(x = Fjob, y = G3)) + geom_point() + geom_smooth(method=lm)

#Arranging all the plots together so that they can be analyzed more easily.
grid.arrange(plot4, plot5, plot6, plot7, plot8, plot9, plot10, plot11, 
             plot12, plot13, plot14, plot15, plot16, plot17, plot18, ncol = 4, 
             nrow = 4, top = "Comparasion of Numeric Predictors with G3")
```
Looking at these graphs, we can see some interesting trends. Failures seems to be the variable that is most indicative of a poor G3 result, as it seems that as the number of failures increase, the lower the G3 score is. This makes sense, because if a student has failed classes in the past, it's more likely that they are not a good student, which would then imply that they don't perform well in their other classes. The other variable that seems to be related to a poor G3 score is age. While I am not as sure for the reason why this is, I present my reasoning for why this happens below. Within a grade level, students are all normally around the same age, give or take about one year. Students who are older than around one year of age are normally the students that are held back a year (or two), due to poor grades. This means that students who are older may perform worse academically, which is why G3 decreases as age increases.

On the other hand, we see that there are some variables that seem to be positively related with the variable G3. For example, Medu and Fedu seem to be strong indicators of a students performance, as the higher the education level of a student's parents, the higher the G3 score is. This makes intuitive sense to me for the following reason. Parents who have a higher education level can provide two benefits to their children. One, as a parent's education level goes up, they may emphasize the importance of education to their children, and push them to perform better in school. Additionally, when a child is struggling, it may be more likely that parent's with high education would help their children out with the material at hand. These are some reasons why I think it makes sense that the variables Fedu and Medu are positively correlated with G3. The other variable that seems to have a somewhat strong positive linear relationship with G3 is studytime, which is pretty self explanatory, as spending more time studying has been shown in the past to improve your grades.

All of the other variables seem to have a slightly positive, slightly negative, or zero slope line. The variables that seem to have the most insignificant effect are as follows: Dalc, Walc, freetime, health, and absences. Due to the graphs above, and the fact that they seem to have no effect on G3, I will not include them in my model as predictors. 



\subsection*{Exploring Binary Categorical Variables vs. G3}
We then want to finish our exploratory data analysis by looking at the relationship between the binary categorical variables and G3. To do this, we will utilize box plots. We create these plots as follows:
```{r, fig.width = 5, fig.height=5}
#Creating the plots for the binary categorical variables against G3.
plot19 <- ggplot(data = dataset, aes(x = G3, y = school, fill = school)) + geom_boxplot(varwidth = TRUE)

plot20 <- ggplot(data = dataset, aes(x = G3, y = sex, fill = sex)) + geom_boxplot(varwidth = TRUE)

plot21 <- ggplot(data = dataset, aes(x = G3, y = address, fill = address)) + geom_boxplot(varwidth = TRUE)

plot22 <- ggplot(data = dataset, aes(x = G3, y = famsize, fill = famsize)) + geom_boxplot(varwidth = TRUE)

plot23 <- ggplot(data = dataset, aes(x = G3, y = Pstatus, fill = Pstatus)) + geom_boxplot(varwidth = TRUE)

plot24 <- ggplot(data = dataset, aes(x = G3, y = schoolsup, fill = schoolsup)) + geom_boxplot(varwidth = TRUE)

plot25 <- ggplot(data = dataset, aes(x = G3, y = famsup, fill = famsup)) + geom_boxplot(varwidth = TRUE)

plot26 <- ggplot(data = dataset, aes(x = G3, y = paid, fill = paid)) + geom_boxplot(varwidth = TRUE)

plot27 <- ggplot(data = dataset, aes(x = G3, y = activities, fill = activities)) + geom_boxplot(varwidth = TRUE)

plot28 <- ggplot(data = dataset, aes(x = G3, y = nursery, fill = nursery)) + geom_boxplot(varwidth = TRUE)

plot29 <- ggplot(data = dataset, aes(x = G3, y = higher, fill = higher)) + geom_boxplot(varwidth = TRUE)

plot30 <- ggplot(data = dataset, aes(x = G3, y = internet, fill = internet)) + geom_boxplot(varwidth = TRUE)

plot31 <- ggplot(data = dataset, aes(x = G3, y = romantic, fill = romantic)) + geom_boxplot(varwidth = TRUE)

#Arranging all the plots together so that they can be analyzed more easily.
grid.arrange(plot19, plot20, plot21, plot22, plot23, plot24, plot25, plot26, 
             plot27, plot28, plot29, plot30, plot31, ncol = 3, nrow = 5,
             top = "Comparasion of Binary Categorical Predictors with G3")
```
Looking at these box plots, we can see that there is a diverse range of positive, neutral, and negative relationships with the outcome variable G3. We begin by discussing box plots that show the binary variable having a positive/negative relationship with G3. 
* First, we note that the address variable (ie, if the student lives in a Rural or Urban area) seems to indicate G3 well. In particular, it seems that students who live in Urban areas perform much better on average than students who live in Rural areas. This makes sense, since students who live in urban areas may have more access to resources and academic help than rural students most of the time. 
* Additionally, we see that the variable higher is also another strong indicator of the a student's G3 value. More specifically, if a student is interested in pursuing higher education, they are much more likely to have a G3 value in the range of 8 to 14, while if they are not interested in pursuing higher education, they are likely to have a G3 value in the range of 0 to 10. This indicates that being interested in higher education is a particularly important variable to determine a student's G3 grade. 
* Finally, a somewhat strong indicator of a student's performance is their access to internet. Students who do have acccess to internet perform better on average thatn students who don't. This is pretty reasonable, as if you have access to internet, you have access to resources that can help you better understand material and perform better in classes.

Based on the plots above, there are also some binary categorical variables that don't seem to have much impact on the results of the outcome variable G3. In particular, they are: famsize, Pstatus, paid, nursery. As a result, we will remove these variables from our final recipe. 


\subsection*{Conclusion of EDA}
Based on the EDA we have performed, we learned quite a few things about our dataset. We saw how the distribution of G3 came to be, by looking at G1 and G2. We learned which variables are most likely to be influential in the prediction of G3 by our graph analysis, and which variables seem to have no impact. Based on all of this, we can now create our recipe, and begin our model fitting.














\section*{Model Fitting}
\subsection*{Final Data Cleaning and Partitioning}
Before we can move on directly to model fitting, we need to perform a few final steps. First, we recall that the variable we are attempting to predict, G3, is a numeric variable, ranging from 0 to 20. We choose to transform this variable into a binary categorical variable, with 0 representing a G3 value from 0 to 13, and 1 representing a G3 value from 14 to 20. In other words, we are redefining G3 into a variable that tells us if a student will pass the Math class or not, based on the idea that a 70 percent in most classes is the cutoff for a passing grade.

One reason I chose to do this is as follows. the score that a student can take on in the class is an integer value ranging from 0 to 20. However, when performing classification techniques such as linear regression, we end up with predicted values that are floats, not integers. While we could still check to see how close the predicted float value is to the actual integer value, I felt that classification would be a better approach for this problem (as opposed to regression).

In the code below, we remove all of the variables we decided to not include from the EDA section, and transform the variable G3 into a categorical variable. 
```{r}
#First, we remove the variables discussed above from dataset.
dataset <- dataset %>% 
  select(-c(G1,G2, Dalc, Walc, freetime, health, absences,
            famsize, Pstatus, paid, nursery))

#Then, we turn the G3 variable into a categorical variable based on the discussion above.
dataset$G3 <- cut(dataset$G3,
                       breaks=c(0, 13, 20),
                       labels=c(0, 1))
```
Having done this, we now want to split our dataset into a training and testing partition. Additionally, we want to fold our training data, so that we can perform cross validation on it to determine the optimal method when fitting our methods. We do this as follows:
```{r}
#splitting the dataset into two for a training and a test set.
dataset_split <- initial_split(dataset, prop = 0.70, strata = G3)

#Creating the training and testing sets.
dataset_train <- training(dataset_split)
dataset_test <- testing(dataset_split)

#Folding the training dataset into ten folds.
dataset_folds <- vfold_cv(dataset_train, v = 10)
```

\subsection*{Recipe Creation}
We now want to create our recipe, which is done below as follows:
```{r class.source = 'fold-show'}
#Creating the Recipe.
recipe <-
  
  #Using all of the remaining predictors to predict G3 on the dataset.
  recipe(G3 ~ ., data = dataset) %>%
  
  #Creating Dummy Variables for all of the Nominal Predictors.
  step_dummy(all_nominal_predictors()) %>%
  
  #Normalizing all of the predictor values.
  step_normalize(all_predictors()) %>%
  
  #Centering all of the predictor values.
  step_center(all_predictors())
```
Now that we have our recipe, we can start to fit our models on the folded data!


\subsection*{Model Fitting}
\subsection*{Logistic Regression and Linear Discriminant Analysis}
I chose to first fit Logistic Regression and Linear Discriminant Analysis. My reasoning for this was as follows: these were the two of the four original models we started to work with in the class, and seemed to be good baselines that we could use for all of the other models we fit. (Note: I also wanted to include QDA and Naive Bayes, but was unable to, due to the models failing when running them on my folded data. This can be explained by the fact that the folds of data do not contain enough observations for QDA and Naive Bayes to be run successfuly). 

For Logistic Regression and Linear Discriminant Analysis, we create the classification object, create the workflow, then fit the model to the folded data. This can be seen below in the code as follows:
```{r}
#LOGISTIC REGRESSION
#Creating the logistic regression object.
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

#Creating the workflow.
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(recipe)

#Fitting the model to the folded data.
fit1 <- fit_resamples(log_wkflow, dataset_folds)


#LINEAR DISCRIMINANT ANALYSIS
#Creating the LDA object.
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#We then define the workflow and add the recipe.
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(recipe)

#We then fit the data on the folded training data.
fit2 <- fit_resamples(lda_wkflow, dataset_folds)
```



\section*{Analyzing the Models}
Having created several different models, we then can check their performance using the collect_metrics() function. 



















\section*{OLD WORK}

Now, we want to get rid of all the nominal predictors in the dataset, and see how it looks like with just numeric predictors. Note that we don't include G1 and G2, since they are clearly related to G3, and we would run into an issue of collinearity.
```{r}
dataset <- dataset %>% select(c(age, Medu, Fedu, traveltime, studytime, 
                                failures, famrel, freetime, goout, Dalc, 
                                Walc, health, absences, G3))
```


Now that we have reduced the dataset just to numerical predictors that shouldn't cause an issue with collinearity, we can perform linear regression. 
```{r}
#Setting the seed.
set.seed(69)


#spliitting the dataset into two for a training and a test set.
dataset_split <- initial_split(dataset, prop = 0.70, strata = G3)

#creating the training and testing sets.
dataset_train <- training(dataset_split)
dataset_test <- testing(dataset_split)

#creating the recipe.
recipe <-
  recipe(G3 ~ ., data = dataset_train) %>%
  step_normalize(all_predictors()) %>%
  step_center(all_predictors())


#creating the linear regression object.
lm_model <- linear_reg() %>%
  set_engine("lm")

#creating the workflow.
lm_wflow <- workflow() %>%
  add_model(lm_model) %>%
  add_recipe(recipe)

#fit the linear model according to our training set.
lm_fit <- fit(lm_wflow, dataset_train)

#viewing the results of this.
results <- lm_fit %>%
  # This will return the parsnip object.
  extract_fit_parsnip() %>%
  # Now we tidy the linear model object.
  tidy()

results
```
Looking at the estimates for each of the variables, we can notice some interesting things. One, it seems that as the variables 1) age, 2) the amount of time you spending travelling to school, 3) the number of previous class failures, and 4) the amount of time you spend going out with friends, increases, the lower your overall class grade will be. On the other hand, we can see that as the 1) the mother's education level, 2) study time, 3) free time, and 4) weekend alcohol consumption increases, the higher your overall class grade will be. 

Most of these trends seem to make sense, with two needing maybe some explanation. I think the reason that the mother's education level being higher leads to an increase in the overall grade, as opposed to the father's, is as follows. Traditinally, mother's are the parents that stay at home with children to take care of them. This means that they spend more time with their children, and help them with things such as homework. Thus, kids who have mother's that are more educated are more likely to have a better education than those that do not. So, if a kid has a mother that is highly educated, than she may be able to help them more in their studies (and get a better grade) than a mother who has not had the chance to obtain a high education. 

The other trend that doesn't make much sense is weekend alcohol consumption level leading to higher grades. My theory is as follows: we can see from the predictors that the higher your weekday alcohol consumption level is, the lower overall grade you might have. Thus, it would make sense that if students who drink a lot on the weekends, but not a lot on the weekdays, have higher final scores, as opposed to students that do the opposite. It might be an indication that students who only drink on the weekends prioritize their studies more, and thus, have higher overall grades.

Using this, we can then see how well this model classifies the observations in the dataset as follows.

```{r}
dataset_metrics <- metric_set(rmse, rsq, mae)

dataset_train_res <- predict(lm_fit, new_data = dataset_train %>% select(-G3))
dataset_train_res <- bind_cols(dataset_train_res, dataset_train %>% select(G3))

metrics <- dataset_metrics(dataset_train_res, truth = G3, estimate = .pred)

metrics
```

From this, we see that the R^2 value is pretty low, which means that only around 15.7 percent of the variability in the G3 value is explained by the predictors. Let's see if we can improve this any further, by perhaps including all of the categorical and nominal predictors in addition to the other predictors.








\section*{Linear Regression Including All Variables}
Now, we want to try doing linear regression, but including all of the variables instead of just the quantative ones. We do this as follows:
```{r}
dataset2 <- read.csv('/Users/vardan/Desktop/pstat131/Final\ Project/student-mat.csv')
dataset2 <- dataset2 %>% select(-c(G1,G2))

#Then, we turn the two nominal categories, Mjob and Fjob, into numeric columns.
dataset2['Mjob'] <- as.numeric(factor(as.matrix(dataset2['Mjob'])))
dataset2['Fjob'] <- as.numeric(factor(as.matrix(dataset2['Fjob'])))


#splitting the dataset into two for a training and a test set.
dataset2_split <- initial_split(dataset2, prop = 0.80, strata = G3)

#creating the training and testing sets.
dataset2_train <- training(dataset2_split)
dataset2_test <- testing(dataset2_split)

#creating the recipe.
recipe2 <-
  recipe(G3 ~ ., data = dataset2_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_center(all_predictors())


#creating the linear regression object.
lm_model2 <- linear_reg() %>%
  set_engine("lm")

#creating the workflow.
lm_wflow2 <- workflow() %>%
  add_model(lm_model2) %>%
  add_recipe(recipe2)

#fit the linear model according to our training set.
lm_fit2 <- fit(lm_wflow2, dataset2_train)

#viewing the results of this.
results2 <- lm_fit2 %>%
  # This will return the parsnip object.
  extract_fit_parsnip() %>%
  # Now we tidy the linear model object.
  tidy()

arrange(results2, estimate)





```
From this, we can see that failures, being in a romantic relationship, going out, and needing extra educational support are some of the stronger factors that impact someone getting a lower final grade. We can also see that mother's education, having a guardian other than the father/mother, and being male leads to a higher grade. Using this, we can then see how well this model classifies the observations in the dataset as follows.

```{r}
dataset_metrics2 <- metric_set(rmse, rsq, mae)

dataset2_train_res <- predict(lm_fit2, new_data = dataset2_train %>% select(-G3))
dataset2_train_res <- bind_cols(dataset2_train_res, dataset2_train %>% select(G3))

metrics2 <- dataset_metrics(dataset2_train_res, truth = G3, estimate = .pred)

metrics2
```
From this, we see that the R^2 value is pretty low, which means that only around 27 percent of the variability in the G3 value is explained by the predictors. Another thing to note is that the predicted values are not integer values, like the actual values are, which we can see below as follows:
```{r}
head(dataset2_train_res)
```

To resolve this, we then try doing logistic regression.

\section*{Logistic Regression Model}
```{r}
#First, we need to split up the response variable, G3, into two categories.
dataset3 <- read.csv('/Users/vardan/Desktop/pstat131/Final\ Project/student-mat.csv')
dataset3 <- dataset3 %>% select(-c(G1,G2))

#Then, we turn the G3 variable into a categorical variable.
#dataset['G3'] <- 

dataset3$G3 <- cut(dataset$G3,
                       breaks=c(0, 10, 20),
                       labels=c(0, 1))


#Then, we turn the two nominal categories, Mjob and Fjob, into numeric columns.
dataset3['Mjob'] <- as.numeric(factor(as.matrix(dataset3['Mjob'])))
dataset3['Fjob'] <- as.numeric(factor(as.matrix(dataset3['Fjob'])))


#splitting the dataset into two for a training and a test set.
dataset3_split <- initial_split(dataset3, prop = 0.80, strata = G3)

#creating the training and testing sets.
dataset3_train <- training(dataset3_split)
dataset3_test <- testing(dataset3_split)

#creating the recipe.
recipe3 <-
  recipe(G3 ~ ., data = dataset3_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_center(all_predictors())


#creating the logistic regression object.
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

#creating the workflow.
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(recipe3)

#fit the logistic model according to our training set.
log_fit <- fit(log_wkflow, dataset3_train)


#viewing the results of this.
results3 <- log_fit %>%
  # This will return the parsnip object.
  extract_fit_parsnip() %>%
  # Now we tidy the linear model object.
  tidy()

arrange(results3, estimate)
```
Then, we want to check the performance of this model.
```{r}
predict(log_fit, new_data = dataset3_train, type = "prob")

augment(log_fit, new_data = dataset3_train) %>%
  conf_mat(truth = G3, estimate = .pred_class)
```
This confusion matrix seems to indicate that we're performing pretty good with this logistic regression model, as opposed to the linear regression model. We calculate it's accuracy on the training dataset as follows:
```{r}
log_reg_acc <- augment(log_fit, new_data = dataset3) %>%
  accuracy(truth = G3, estimate = .pred_class)
log_reg_acc
```
It seems that we have approximately 76.5 percent training accuracy on this dataset. That's pretty good! We then test it on the testing data to see how it performs there.

```{r}
#Predicting on the testing data.
predict(log_fit, new_data = dataset3_test, type = "prob")

augment(log_fit, new_data = dataset3_test) %>%
  conf_mat(truth = G3, estimate = .pred_class)

multi_metric <- metric_set(accuracy, sensitivity, specificity)

augment(log_fit, new_data = dataset3_test) %>%
  multi_metric(truth = G3, estimate = .pred_class)
```
It looks like we have 61.76 accuracy, which is somewhat good, although not great. We then want to see how the other types of models we learned in class perform. We use the LDA, QDA, and Naive Bayes Models on the training data and check their accuracies. 
```{r}
#LDA WORK.
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#We then define the workflow and add the recipe.
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(recipe3)

#We then fit the data on the training data.
lda_fit <- fit(lda_wkflow, dataset3_train)

#getting the accuracy
lda_acc <- augment(lda_fit, new_data = dataset3_train) %>%
  accuracy(truth = G3, estimate = .pred_class)

#QDA WORK.
#Defining the model for QDA.
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#Creating the workflow and adding the recipe.
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(recipe3)

#Fitting the model.
qda_fit <- fit(qda_wkflow, dataset3_train)

qda_acc <- augment(qda_fit, new_data = dataset3_train) %>%
  accuracy(truth = G3, estimate = .pred_class)

#NAIVE BAYES WORK.
#We fit the naive Bayes model as asked of us.
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

#We create the workflow, and add our recipe into it.
nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(recipe3)

#We then fit our model on the training dataset.
nb_fit <- fit(nb_wkflow, dataset3_train)

#Getting the Naive Bayes accuracy.
nb_acc <- augment(nb_fit, new_data = dataset3_train) %>%
  accuracy(truth = G3, estimate = .pred_class)

#Getting the accuracy for logistic regression.
#log_reg_acc <- augment(log_fit, new_data = dataset3_train) %>%
#  accuracy(truth = G3, estimate = .pred_class)


#Putting all of the accuracies together.
accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)

#Putting all of the model names together.
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")

#Combining the two lists into one tibble.
results <- tibble(accuracies = accuracies, models = models)

#Printing out the results and arranging them nicely. 
results %>% 
  arrange(-accuracies)
```
Looking at this, we can see that on the training data, QDA seems to be performing the best, at a whopping 92.73 percent. Logistic Regression, LDA, and Naive Bayes all seem to be performing very closely to one another, ranging from 76.47 percent to 73.66 percent to 71.62 percent, respectively. However, this is all on the training data. Let us see how they perform on the testing data.
```{r}
#ISSUE CURRENTLY: CANNOT DO THIS ON TRAINING DATA BECAUSE ITS TOO SMALL?




#LDA WORK.
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#We then define the workflow and add the recipe.
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(recipe3)

#We then fit the data on the training data.
lda_fit <- fit(lda_wkflow, dataset3_test)

#getting the accuracy
lda_acc <- augment(lda_fit, new_data = dataset3_test) %>%
  accuracy(truth = G3, estimate = .pred_class)

#QDA WORK.
#Defining the model for QDA.
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

#Creating the workflow and adding the recipe.
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(recipe3)

#Fitting the model.
qda_fit <- fit(qda_wkflow, dataset3_test)

qda_acc <- augment(qda_fit, new_data = dataset3_test) %>%
  accuracy(truth = G3, estimate = .pred_class)

#NAIVE BAYES WORK.
#We fit the naive Bayes model as asked of us.
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

#We create the workflow, and add our recipe into it.
nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(recipe3)

#We then fit our model on the training dataset.
nb_fit <- fit(nb_wkflow, dataset3_test)

#Getting the Naive Bayes accuracy.
nb_acc <- augment(nb_fit, new_data = dataset3_test) %>%
  accuracy(truth = G3, estimate = .pred_class)


#fit the logistic model according to our testing set.
log_fit <- fit(log_wkflow, dataset3_test)

#Getting the accuracy for logistic regression.
log_reg_acc <- augment(log_fit, new_data = dataset3_test) %>%
  accuracy(truth = G3, estimate = .pred_class)


#Putting all of the accuracies together.
accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)

#Putting all of the model names together.
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")

#Combining the two lists into one tibble.
results <- tibble(accuracies = accuracies, models = models)

#Printing out the results and arranging them nicely. 
results %>% 
  arrange(-accuracies)
```
We can see that all of our methods seem to perform quite a bit worse on the testing data, as opposed to the training data. Suprisingly, we see that Naive Bayes seems to perform the best, with a 70.58 percent, which is quite close to the 71.62 percent it reached on the training data. On the other hand, all the other methods are performing about 10 percent worse, with the exception of QDA, which is performing about 25 percent worse. Finally, while accuracy is a decent indicator of the performance of a model, we can also use the ROC curve to determine if a model is performing well or not. 



























